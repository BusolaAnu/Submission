Optimizing an ML Pipeline in Azure

Overview
This project is part of the Udacity Azure ML Nanodegree. In this project, we build and optimize an Azure ML pipeline using the Python SDK and a provided Scikit-learn model. This model is then compared to an Azure AutoML run.

Summary
This dataset contains data about a bank’s marketing campaign result and we seek to predict whether a customer subscribe to term deposit (yes) or not (no).

"The best performing model was VotingEnsemble with an accuracy of 91.51%. This is an ensemble the following models: 
[MaxAbsScaler LightGBM, StandardScalerWrapper LightGBM, TruncatedSVDWrapper XGBoost, MaxAbsScaler XGBoost].

HyperDrive Config

Scikit-learn Pipeline
The purpose of the pipeline is to assemble several steps that can be cross-validated together while setting different parameters.

The SKLearn Pipeline goes through the following steps:
•	Get the training data.
•	Clean/preprocess/transform the data.
•	Train the machine learning model using logistic regression.
•	Evaluate and optimize the model.
•	Clean/preprocess/transform test data.
•	Fit the model on test data to make predictions.

Classification algorithm is a Supervised Learning technique that is used to identify the category of new observations based on learnings from the training data. In Classification, a program learns from the given dataset or observations and then classifies new observation into a number of classes or groups.

**What are the benefits of the parameter sampler you chose?**

Hyperparameter tuning is the process of finding the configuration of hyperparameters that results in the best performance. The hyperparameter sampling used for this project was Random Parameter Sampling with number of hidden layers ranging from 1 to 5 and batch size 16, 32, 64 and 128.

Benefits of Random parameter sampler chosen for this project is that hyperparameter values are randomly selected from the defined search space thereby reducing computational time unlike Grid Parameter Sampling that explores the entire parameter search space.
Also for Random parameter sampling, the number of runs can be controlled and it allows for sampling a range of parameters that can be further refined with a Grid parameter sampling.

**What are the benefits of the early stopping policy you chose?**
	
Early termination improves computational efficiency by automatically ending poor performing runs with an early stopping policy.
Bandit early stopping policy was used in this project and it is based on slack factor/slack amount and evaluation interval. The policy early terminates any runs where the primary metric is not within the specified slack factor/slack amount with respect to the best performing training run.

AutoML

The best performing model for AutoML was VotingEnsemble.

Ensembled algorithms: [MaxAbsScaler LightGBM, StandardScalerWrapper LightGBM, StandardScalerWrapper LightGBM, TruncatedSVDWrapper XGBoost, MaxAbsScaler LightGBM, MaxAbsScaler LightGBM, TruncatedSVDWrapper XGBoost, MaxAbsScaler LightGBM, MaxAbsScaler XGBoost, MaxAbsScaler LightGBM].

Ensembled weights: [0.15384615384615385, 0.07692307692307693, 0.07692307692307693, 0.07692307692307693, 0.07692307692307693, 0.15384615384615385, 0.15384615384615385, 0.07692307692307693, 0.07692307692307693, 0.07692307692307693]

The hyperparameters generated by the AutoML includes:
reg_alpha=1.4583333333333335, reg_lambda=1.4583333333333335, subsample=1, tree_method='auto', is_cross_validation=True

Pipeline comparison
AutoML pipeline performed slightly better than the scikit learn pipeline with the accuracy of the AutoML being 91.51 % and 91.27% for scikit learn.

Future work

Some areas of improvement for the project is to change the range of values selected for the hyperparameters to test for a better result. 

Selecting a different parameter sampler and early stopping policy to see the effect on performance.
