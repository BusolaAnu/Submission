**Optimizing an ML Pipeline in Azure**

**Overview**
This project is part of the Udacity Azure ML Nanodegree. In this project, we build and optimize an Azure ML pipeline using the Python SDK and a provided Scikit-learn model using logistic regression classification algorithm. This model is then compared to an Azure AutoML run.

**Summary**
This dataset contains data about a bank’s marketing campaign result and we seek to predict whether a customer subscribe to term deposit (yes) or not (no).

"The best performing model was VotingEnsemble with an accuracy of 91.51%. This is an ensemble the following models: 
[MaxAbsScaler LightGBM, StandardScalerWrapper LightGBM, TruncatedSVDWrapper XGBoost, MaxAbsScaler XGBoost].

**HyperDrive Config**

**Scikit-learn Pipeline**
The purpose of the pipeline is to assemble several steps that can be cross-validated together while setting different parameters.

The SKLearn Pipeline goes through the following steps:
•	Get the training data.
•	Clean/preprocess/transform the data.
•	Train the machine learning model using logistic regression.
•	Evaluate and optimize the model.
•	Clean/preprocess/transform test data.
•	Fit the model on test data to make predictions.
*Reference: https://www.analyticsvidhya.com/blog/2020/01/build-your-first-machine-learning-pipeline-using-scikit-learn/ 

Classification algorithm is a Supervised Learning technique that is used to identify the category of new observations based on learnings from the training data. In Classification, a program learns from the given dataset or observations and then classifies new observation into several classes or groups.
*Reference: https://www.javatpoint.com/classification-algorithm-in-machine-learning.
The Classification Algorithm used in this project was Logistic regression and the parameters tuned were Regularisation Strength (--C) and Maximum Iteration

**Benefits of Random Parameter Sampling**

Hyperparameter tuning is the process of finding the configuration of hyperparameters that results in the best performance. The hyperparameter sampling used for this project was Random Parameter Sampling with number of hidden layers ranging from 1 to 5 and batch size 16, 32, 64 and 128.

Benefits of Random parameter sampler chosen for this project is that hyperparameter values are randomly selected from the defined search space thereby reducing computational time unlike Grid Parameter Sampling that explores the entire parameter search space.
Also for Random parameter sampling, the number of runs can be controlled and it allows for sampling a range of parameters that can be further refined with a Grid parameter sampling.
*Reference: https://githubmemory.com/repo/apollo2030/automl-pipeline 

**Benefits of Bandit early stopping policy**
	
1. Early termination improves computational efficiency by automatically ending poor performing runs with an early stopping policy.
2. Bandit early stopping policyis based on slack factor/slack amount and evaluation interval. 
3. The policy early terminates any runs where the primary metric is not within the specified slack factor/slack amount with respect to the best performing training run.
*Reference: https://azure.github.io/azureml-sdk-for-r/reference/bandit_policy.html

**AutoML**

The best performing model for AutoML was VotingEnsemble.
The ensembled model and their weights are as follows.

**Ensembled Algorithm		Weights**
MaxAbsScaler LightGBM: 		0.15384615384615385
StandardScalerWrapper LightGBM: 0.07692307692307693
StandardScalerWrapper LightGBM: 0.15384615384615385
TruncatedSVDWrapper XGBoost:  	0.07692307692307693
MaxAbsScaler LightGBM: 		0.07692307692307693
MaxAbsScaler LightGBM: 		0.07692307692307693
TruncatedSVDWrapper XGBoost: 	0.07692307692307693
MaxAbsScaler LightGBM: 		0.15384615384615385
MaxAbsScaler XGBoost: 		0.15384615384615385
MaxAbsScaler LightGBM: 		0.07692307692307693
 
**
The hyperparameters generated by the AutoML includes:**
reg_alpha=1.4583333333333335, reg_lambda=1.4583333333333335, subsample=1, tree_method='auto', is_cross_validation=True

**Pipeline comparison**
AutoML pipeline performed slightly better than the scikit learn pipeline with the accuracy of the AutoML being 91.51 % and 91.27% for scikit learn,
This could result from the different algorithms the AutoML used to train the model unlike the scikit learn that used only logistic regression algorithm.
**
Future work**

Some areas of improvement for the project is to change the range of values selected for the hyperparameters regularisation strength and max iteration to test for a better result. 

Selecting a different parameter sampler and early stopping policy to see the effect on performance.
